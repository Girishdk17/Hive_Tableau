Creating a bucket 
Make it public 
Make Raw and Clean zone directories
Policies :
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::awsexamplebucket/*",
            "Condition": {
                "StringEquals": {
                    "s3:ExistingObjectTag/public": "yes"
                }
            }
        }
    ]
}



Downloading the Data set from kaggle
https://www.kaggle.com/yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018
https://www.kaggle.com/usdot/flight-delays
Make this data Downloading process automate through EMR
Logging KAGGLE with your account details and download your account credential :
Go to MyAccount and make new API token download as .json
Go to the required data set and copy API Command for that dataset
kaggle datasets download -d usdot/flight-delays
kaggle datasets download -d yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018
Making script
Bootstrap .sh  : when cluster is in bootstrap mode that time this download all the dependency for our requirement 

#!/bin/bash

sudo ln -fs /usr/bin/python3.6 /etc/alternatives/python
sudo ln -fs /usr/bin/pip-3.6 /etc/alternatives/pip
sudo pip install --upgrade pip 
sudo pip install kaggle-cli
sudo pip install kaggle

aws s3 cp s3://grstechmate/kaggle.json /home/hadoop/kaggle.json


Shell script to download data and put it into S3 bucket RAW data folder
#!/bin/bash

cd /home/hadoop/

mkdir /home/hadoop/.kaggle
sudo cp /home/hadoop/kaggle.json /home/hadoop/.kaggle/

/usr/local/bin/pip3.6 install --user kaggle
/usr/local/bin/pip3.6 install kaggle
~/.local/bin/kaggle datasets download -d yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018 --path /home/hadoop/Raw_09to16 --unzip

~/.local/bin/kaggle datasets download -d usdot/flight-delays --path /home/hadoop/Airport_airline --unzip


sudo mkdir RDS
sudo cp Raw_09to16/2017.csv RDS/
sudo mkdir INC
sudo cp Raw_09to16/2018.csv INC/
sudo rm Raw_09to16/2017.csv
sudo rm Raw_09to16/2018.csv
sudo mkdir Airlines
sudo mkdir Airports
sudo cp Airport_airline/airports.csv  Airports/
sudo cp Airport_airline/airlines.csv  Airlines/

sudo mkdir GROUP6_RawHub

sudo cp -a Airlines GROUP6_RawHub/
sudo cp -a Airports GROUP6_RawHub/
sudo cp -a RDS GROUP6_RawHub/
sudo cp -a INC GROUP6_RawHub/
sudo cp -a Raw_09to16 GROUP6_RawHub/

hadoop fs -put /home/hadoop/GROUP6_RawHub

s3-dist-cp --src /user/hadoop/GROUP6_RawHub --dest s3://hive-vita/Raw_zone/GROUP6_RawHub/

Clean The data using Spark
import pyspark
from pyspark.sql.types import *
from pyspark.context import SparkContext
from pyspark.sql.session import SparkSession

sc = SparkContext('local')
spark = SparkSession(sc)


df=spark.read.csv("s3a://hive-vita/Raw_zone/GROUP6_RawHub/Raw_09to16/",inferSchema=True,header=True)
df=df.drop('Unnamed: 27')


from pyspark.sql import functions as f
df=df.withColumn("FL_DATE", f.from_unixtime(f.unix_timestamp(df.FL_DATE), "yyyy-MM-dd"))

import pyspark.sql.functions as f
df_split = f.split(df['FL_DATE'], '-')

df = df.withColumn('year', df_split.getItem(0))
df = df.withColumn('month', df_split.getItem(1))
df = df.withColumn('day', df_split.getItem(2))


df1=df.na.fill({'DEP_TIME': 0,'DEP_DELAY': 0,'TAXI_OUT': 0,'WHEELS_OFF': 0,'WHEELS_ON': 0,'CANCELLATION_CODE': 'F','TAXI_IN': 0,'ARR_TIME': 0,'ARR_DELAY': 0,'ACTUAL_ELAPSED_TIME': 0,'AIR_TIME': 0,'CARRIER_DELAY': 0, 'WEATHER_DELAY': 0,'NAS_DELAY': 0,'SECURITY_DELAY': 0,'LATE_AIRCRAFT_DELAY': 0})

from pyspark.sql.functions import col, when
df1=df1.withColumn('delay', when(df1.ARR_DELAY > 0, df1.ARR_DELAY).otherwise(0))
df1=df1.withColumn('status', when(df1.ARR_DELAY > 0, 1).otherwise(0))



from pyspark.sql.types import IntegerType,DoubleType,DateType

df1 = df1.withColumn("FL_DATE", df1["FL_DATE"].cast(DateType()))
df1 = df1.withColumn("year", df1["year"].cast(IntegerType()))
df1 = df1.withColumn("month", df1["month"].cast(IntegerType()))
df1 = df1.withColumn("day", df1["day"].cast(IntegerType()))


df2 = df1.withColumnRenamed("FL_DATE","fl_date").withColumnRenamed("OP_CARRIER","op_carrier").withColumnRenamed("OP_CARRIER_FL_NUM","op_carrier_fl_num").withColumnRenamed("OP_CARRIER_FL_NUM","op_carrier_fl_num").withColumnRenamed("ORIGIN","origin").withColumnRenamed("DEST","dest").withColumnRenamed("CRS_DEP_TIME","crs_dep_time").withColumnRenamed("DEP_TIME","dep_time").withColumnRenamed("DEP_DELAY","dep_delay").withColumnRenamed("TAXI_OUT","taxi_out").withColumnRenamed("WHEELS_OFF","wheels_off").withColumnRenamed("WHEELS_ON","wheels_on").withColumnRenamed("TAXI_IN","taxi_in").withColumnRenamed("CRS_ARR_TIME","crs_arr_time").withColumnRenamed("ARR_TIME","arr_time").withColumnRenamed("ARR_DELAY","arr_delay").withColumnRenamed("CANCELLED","cancelled").withColumnRenamed("CANCELLATION_CODE","cancellation_code").withColumnRenamed("DIVERTED","diverted").withColumnRenamed("CRS_ELAPSED_TIME","crs_elapsed_time").withColumnRenamed("ACTUAL_ELAPSED_TIME","actual_elapsed_time").withColumnRenamed("AIR_TIME","air_time").withColumnRenamed("DISTANCE","distance").withColumnRenamed("CARRIER_DELAY","carrier_delay").withColumnRenamed("WEATHER_DELAY","whather_delay").withColumnRenamed("NAS_DELAY","nas_delay").withColumnRenamed("SECURITY_DELAY","security_delay").withColumnRenamed("LATE_AIRCRAFT_DELAY","late_aircraft_delay")

#df2.write.format("orc").save("s3a://hive-vita/Clean_zone/Flight_logs/")

df2.write.format("orc").mode("append").save("s3a://hive-vita/Clean_zone/Flight_logs/")

RDS
Add Jar 
#!/bin/bash

cd /home/hadoop/


# Read csv from S3

# Write csv to RDS

#reading from database

#converting FL_DATE to unixtime

#spliting FL_DATE to get year,month day columns

#filling NA values

#adding status and delay column to dataframe

#converting datatype of columns

#changinge columns name
f2.write.format("orc").mode("append").save("s3a://finalgroup6/clean_data/final26a/")

Hive Data WareHouse
Create Table in Hive
##---CREATE A TABLE OF RAW DATA---##

Making all this process Automated by using CFT and Step

Making of Step 
