Creating a bucket 
Make it public 
Make Raw and Clean zone directories
Policies :
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::awsexamplebucket/*",
            "Condition": {
                "StringEquals": {
                    "s3:ExistingObjectTag/public": "yes"
                }
            }
        }
    ]
}

Downloading the Data set from kaggle
https://www.kaggle.com/yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018
https://www.kaggle.com/usdot/flight-delays
Make this data Downloading process automate through EMR
Logging KAGGLE with your account details and download your account credential :
Go to MyAccount and make new API token download as .json
Go to the required data set and copy API Command for that dataset
kaggle datasets download -d usdot/flight-delays
kaggle datasets download -d yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018
Making script
Bootstrap .sh  : when cluster is in bootstrap mode that time this download all the dependency for our requirement 

Shell script to download data and put it into S3 bucket RAW data folder
#!/bin/bash

RDS
Add Jar 
#!/bin/bash 

cd /home/hadoop/


# Read csv from S3

# Write csv to RDS

#reading from database

#converting FL_DATE to unixtime

#spliting FL_DATE to get year,month day columns

#filling NA values

#adding status and delay column to dataframe

#converting datatype of columns

#changinge columns name
f2.write.format("orc").mode("append").save("s3a://finalgroup6/clean_data/final26a/")

Hive Data WareHouse
Create Table in Hive
##---CREATE A TABLE OF RAW DATA---##

Making all this process Automated by using CFT and Step

Making of Step 
